{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.framework import graph_util\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 60\n",
    "h = 80\n",
    "BATCH_SIZE = 5\n",
    "DATA_SIZE = w*h\n",
    "LAYER_H = 64\n",
    "OUT_NUM = 10\n",
    "STDDEV = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load4csv(filename, size=2048, k=255.0):\n",
    "\n",
    "    dataframe = pd.read_csv(filename)\n",
    "    feature_cols = dataframe.columns[:-1] # all but image column #去除最后一列数据，最后一列为image\n",
    "    # transform image space-separated pixel values to normalized pixel vector\n",
    "\n",
    "    dataframe['data'] = dataframe['data'].apply(lambda img: np.fromstring(img, sep = ' ') / k)\n",
    "    dataframe = dataframe.dropna() # drop entries w/NaN entries\n",
    "    X = np.vstack(dataframe['data'])\n",
    "    X = X.reshape(-1, size, 1)\n",
    "    \n",
    "    y = dataframe[feature_cols].values\n",
    "    \n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = load4csv('data.csv', w*h, 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)#平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)#标准差\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))#最大值\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))#最小值\n",
    "        tf.summary.histogram('histogram', var)#直方图\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, DATA_SIZE], name='x')\n",
    "    variable_summaries(x)\n",
    "    y = tf.placeholder(tf.float32, [None, OUT_NUM], name='y')\n",
    "    variable_summaries(y)\n",
    "    k = tf.placeholder(tf.float32, name='k')\n",
    "    variable_summaries(k)\n",
    "    \n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('h1'):\n",
    "        with tf.name_scope('v'):\n",
    "            h1w = tf.Variable(tf.truncated_normal([DATA_SIZE, LAYER_H], stddev=STDDEV), name='h1w')\n",
    "            variable_summaries(h1w)\n",
    "            h1b = tf.Variable(tf.constant(STDDEV, shape=[LAYER_H]), name='h1b')\n",
    "            variable_summaries(h1b)\n",
    "        with tf.name_scope('p'):\n",
    "            dense = tf.reshape(x, [-1, DATA_SIZE], name='dense')\n",
    "            variable_summaries(dense)\n",
    "            h1 = tf.nn.relu(tf.matmul(dense, h1w) + h1b)\n",
    "            variable_summaries(h1)\n",
    "            h1drop = tf.nn.dropout(h1, k)\n",
    "            variable_summaries(h1drop)\n",
    "    with tf.name_scope('out'):\n",
    "        with tf.name_scope('v'):\n",
    "            low = tf.Variable(tf.truncated_normal([LAYER_H, OUT_NUM], stddev=STDDEV), name='low')\n",
    "            variable_summaries(low)\n",
    "            lob = tf.Variable(tf.constant(STDDEV, shape=[OUT_NUM]), name='lob')\n",
    "            variable_summaries(lob)\n",
    "        with tf.name_scope('p'):\n",
    "            y_out = tf.nn.softmax(tf.add(tf.matmul(h1drop, low), lob), name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-y_out), 1))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "with tf.name_scope('train'):\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate, 0.95).minimize(loss, global_step=global_step)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_out, 1))\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "init = tf.global_variables_initializer()\n",
    "merged = tf.summary.merge_all()\n",
    "print(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.389523 0.177723 0.0103468 0.0181601 0.144399 0.131836 0.000961292 0.0299836 0.00301592 0.094051\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 1.182175 training accuracy 0.000000\n",
      "-------------\n",
      "10\n",
      "0.167478 0.0974909 0.081637 0.12365 0.131275 0.195007 0.0336139 0.0933664 0.0172013 0.0592812\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.900823 training accuracy 0.400000\n",
      "-------------\n",
      "20\n",
      "0.0635482 0.0317253 0.0137053 0.0385633 0.0371939 0.640821 0.0930483 0.0350277 0.00796145 0.0384052\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.507926 training accuracy 0.600000\n",
      "-------------\n",
      "30\n",
      "0.0251238 0.0353995 0.00712633 0.0203148 0.025374 0.70572 0.0981481 0.0253894 0.00795969 0.0494441\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.235410 training accuracy 1.000000\n",
      "-------------\n",
      "40\n",
      "0.0138387 0.0156179 0.0025536 0.0094443 0.00923056 0.854468 0.0603906 0.0146094 0.00441694 0.01543\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.089390 training accuracy 1.000000\n",
      "-------------\n",
      "50\n",
      "0.00657696 0.0143685 0.00172469 0.00316316 0.00925103 0.887925 0.0380966 0.0112628 0.0103892 0.0172425\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.038651 training accuracy 1.000000\n",
      "-------------\n",
      "60\n",
      "0.00387867 0.00882252 0.00104508 0.00135393 0.00699708 0.917314 0.0264323 0.00966435 0.00725789 0.0172339\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.022003 training accuracy 1.000000\n",
      "-------------\n",
      "70\n",
      "0.00306543 0.00702448 0.000688142 0.00169785 0.0049348 0.950004 0.0149338 0.00658343 0.00501412 0.00605342\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.007934 training accuracy 1.000000\n",
      "-------------\n",
      "80\n",
      "0.00412443 0.00889833 0.000618684 0.00146924 0.00445056 0.94159 0.0185494 0.00490174 0.00838568 0.00701194\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.008136 training accuracy 1.000000\n",
      "-------------\n",
      "90\n",
      "0.00217243 0.00493642 0.000679297 0.00125399 0.00373697 0.950741 0.0196744 0.00447501 0.00542561 0.00690515\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.007208 training accuracy 1.000000\n",
      "-------------\n",
      "100\n",
      "0.00160693 0.00409614 0.000441508 0.00154922 0.00345084 0.957428 0.0122653 0.00463555 0.00385736 0.0106689\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.001852 training accuracy 1.000000\n",
      "-------------\n",
      "110\n",
      "0.000953423 0.00328491 0.000354266 0.00064815 0.00247217 0.977033 0.00591917 0.00233955 0.00368213 0.00331275\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.003107 training accuracy 1.000000\n",
      "-------------\n",
      "120\n",
      "0.00131215 0.00392304 0.000363854 0.00156353 0.00272099 0.971835 0.00895763 0.00285023 0.00246106 0.00401272\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000820 training accuracy 1.000000\n",
      "-------------\n",
      "130\n",
      "0.000905033 0.00202587 0.000247614 0.00186126 0.00251378 0.982361 0.00535269 0.00170292 0.00140278 0.00162716\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000665 training accuracy 1.000000\n",
      "-------------\n",
      "140\n",
      "0.000445971 0.00224695 9.65801e-05 0.000484605 0.0017183 0.989408 0.00204914 0.0012604 0.00117537 0.00111437\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000578 training accuracy 1.000000\n",
      "-------------\n",
      "150\n",
      "0.000557203 0.00164981 0.000220974 0.000690881 0.00181546 0.983672 0.00543272 0.00177561 0.00160697 0.00257851\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000442 training accuracy 1.000000\n",
      "-------------\n",
      "160\n",
      "0.00031481 0.00194888 0.000154858 0.000244507 0.00155839 0.984173 0.00631183 0.000964315 0.00177557 0.00255362\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.001136 training accuracy 1.000000\n",
      "-------------\n",
      "170\n",
      "0.000213562 0.000794021 7.09457e-05 0.000369563 0.00134335 0.993125 0.00148678 0.000840644 0.000966852 0.000789276\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000310 training accuracy 1.000000\n",
      "-------------\n",
      "180\n",
      "0.000317108 0.000865567 5.67776e-05 0.00027194 0.0014337 0.992355 0.00226315 0.000630127 0.000699564 0.00110741\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000229 training accuracy 1.000000\n",
      "-------------\n",
      "190\n",
      "0.000522465 0.00222927 0.00011552 0.000542756 0.00191326 0.986961 0.00420482 0.00100723 0.00142588 0.00107808\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000281 training accuracy 1.000000\n",
      "-------------\n",
      "200\n",
      "0.000167638 0.00109766 7.49212e-05 0.000218701 0.000925567 0.993391 0.0020701 0.000615916 0.000753029 0.000685188\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000248 training accuracy 1.000000\n",
      "-------------\n",
      "210\n",
      "0.000329307 0.00167444 0.000108104 0.00051405 0.00133635 0.989995 0.00237022 0.00105719 0.00143017 0.00118559\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000217 training accuracy 1.000000\n",
      "-------------\n",
      "220\n",
      "0.00010854 0.0006574 4.14404e-05 0.000114405 0.00104583 0.995622 0.00104512 0.000271686 0.000497995 0.000595618\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000268 training accuracy 1.000000\n",
      "-------------\n",
      "230\n",
      "0.000337681 0.000844962 5.40845e-05 0.000272283 0.00167493 0.990972 0.00216148 0.00158597 0.000702537 0.00139404\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000115 training accuracy 1.000000\n",
      "-------------\n",
      "240\n",
      "9.78984e-05 0.000521637 4.86946e-05 0.000157238 0.00143557 0.994289 0.00124298 0.000652398 0.00050652 0.00104762\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000169 training accuracy 1.000000\n",
      "-------------\n",
      "250\n",
      "0.00026169 0.000877617 5.04629e-05 0.00026989 0.0010738 0.991879 0.00243162 0.000985837 0.00105435 0.00111537\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000110 training accuracy 1.000000\n",
      "-------------\n",
      "260\n",
      "0.000333352 0.000616893 1.88547e-05 0.000190479 0.000941642 0.995714 0.000738087 0.000541485 0.000583952 0.000321116\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000068 training accuracy 1.000000\n",
      "-------------\n",
      "270\n",
      "0.000159878 0.000525167 4.66693e-05 0.000141258 0.00106702 0.994777 0.00134319 0.000668935 0.000567127 0.000703538\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000105 training accuracy 1.000000\n",
      "-------------\n",
      "280\n",
      "0.000102624 0.000512391 3.22494e-05 0.000155821 0.000856567 0.995212 0.0015246 0.000409657 0.000572231 0.00062201\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000071 training accuracy 1.000000\n",
      "-------------\n",
      "290\n",
      "6.75611e-05 0.000595311 1.8189e-05 0.000108185 0.000619305 0.996837 0.000823617 0.000283594 0.000411544 0.000235188\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000071 training accuracy 1.000000\n",
      "-------------\n",
      "300\n",
      "0.000151345 0.000599627 3.31006e-05 0.00011113 0.000617619 0.994779 0.00177889 0.000448173 0.000862949 0.000617909\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000052 training accuracy 1.000000\n",
      "-------------\n",
      "310\n",
      "4.6849e-05 0.000368478 2.30604e-05 0.000102495 0.000848734 0.99655 0.000949687 0.000163365 0.0003978 0.000549504\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000075 training accuracy 1.000000\n",
      "-------------\n",
      "320\n",
      "7.42701e-05 0.000365662 2.02762e-05 0.000180828 0.000642017 0.997521 0.000367054 0.000296458 0.000330835 0.000201615\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000042 training accuracy 1.000000\n",
      "-------------\n",
      "330\n",
      "5.1438e-05 0.000312533 1.29528e-05 4.16593e-05 0.000741502 0.99726 0.00074676 0.000244227 0.000285297 0.00030392\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000152 training accuracy 1.000000\n",
      "-------------\n",
      "340\n",
      "0.000213069 0.000861169 2.26868e-05 0.000284637 0.00071885 0.995731 0.00119421 0.000274281 0.000418058 0.000281519\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000037 training accuracy 1.000000\n",
      "-------------\n",
      "350\n",
      "4.15982e-05 0.000452958 1.95181e-05 4.1255e-05 0.000359252 0.996538 0.00135704 0.000244036 0.000519373 0.000427386\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000157 training accuracy 1.000000\n",
      "-------------\n",
      "360\n",
      "0.000115283 0.000414716 3.48936e-05 0.000167285 0.000745692 0.994149 0.00171114 0.000762335 0.000666619 0.0012331\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000033 training accuracy 1.000000\n",
      "-------------\n",
      "370\n",
      "6.95157e-05 0.000314138 9.68585e-06 0.000392747 0.00176133 0.996411 0.000200111 0.000504166 0.000166271 0.000171473\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000009 training accuracy 1.000000\n",
      "-------------\n",
      "380\n",
      "7.27182e-05 0.000587725 2.50969e-05 8.55442e-05 0.000804815 0.994268 0.0028112 0.000466069 0.000481034 0.00039822\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000024 training accuracy 1.000000\n",
      "-------------\n",
      "390\n",
      "1.0286e-05 0.00024272 1.97135e-05 2.88086e-05 0.000363831 0.997426 0.00101053 0.000149743 0.000348656 0.000399302\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000065 training accuracy 1.000000\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "4.04478e-05 0.000233673 1.80918e-05 9.47914e-05 0.000361456 0.997914 0.000376467 0.000164943 0.000640301 0.000155808\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000017 training accuracy 1.000000\n",
      "-------------\n",
      "410\n",
      "1.80503e-05 0.000112435 8.9458e-06 2.15565e-05 0.00030982 0.998946 0.000232985 9.22336e-05 0.000116061 0.000141671\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000015 training accuracy 1.000000\n",
      "-------------\n",
      "420\n",
      "4.20949e-05 0.000463988 1.85428e-05 9.10759e-05 0.000258686 0.997882 0.000484698 0.000133597 0.000408007 0.000216801\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000027 training accuracy 1.000000\n",
      "-------------\n",
      "430\n",
      "1.40318e-05 0.000172879 1.4502e-05 9.81373e-05 0.00036395 0.998646 0.000261195 8.74361e-05 0.000154736 0.000187132\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000003 training accuracy 1.000000\n",
      "-------------\n",
      "440\n",
      "1.53313e-05 0.000145261 6.9186e-06 6.06517e-05 0.000726255 0.998284 0.000188115 0.000187238 0.000234681 0.000151536\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000007 training accuracy 1.000000\n",
      "-------------\n",
      "450\n",
      "7.45417e-05 0.000277709 1.11051e-05 3.305e-05 0.000445339 0.997279 0.000662523 0.000166632 0.000457723 0.000592191\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000098 training accuracy 1.000000\n",
      "-------------\n",
      "460\n",
      "3.61779e-05 0.000303727 3.21769e-05 0.00017438 0.000700106 0.99657 0.00108795 0.000289338 0.000488388 0.000317366\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000010 training accuracy 1.000000\n",
      "-------------\n",
      "470\n",
      "1.81242e-05 0.000155486 1.17232e-05 3.77679e-05 0.000506951 0.998265 0.000334734 0.000174211 0.000327223 0.000168564\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000005 training accuracy 1.000000\n",
      "-------------\n",
      "480\n",
      "3.60225e-05 0.000100679 7.18927e-06 5.18534e-05 0.000557498 0.998692 0.000143858 0.00015791 0.000124064 0.000128558\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000004 training accuracy 1.000000\n",
      "-------------\n",
      "490\n",
      "2.32469e-05 0.000129048 6.98297e-06 7.36924e-05 0.000661449 0.997968 0.00050965 0.000290748 0.000192829 0.000144092\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "500\n",
      "1.23498e-05 0.00014724 8.55946e-06 2.51609e-05 0.000216388 0.998897 0.000235254 7.0743e-05 0.000294181 9.33839e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000018 training accuracy 1.000000\n",
      "-------------\n",
      "510\n",
      "4.10473e-05 0.000189381 9.25955e-06 3.3602e-05 0.000292349 0.998187 0.000728704 0.000157571 0.00017846 0.000182944\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "520\n",
      "6.74424e-05 9.22406e-05 1.27581e-05 0.00073747 0.00268075 0.995398 0.000361247 0.000398692 6.90969e-05 0.000182411\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000007 training accuracy 1.000000\n",
      "-------------\n",
      "530\n",
      "2.71781e-05 9.88941e-05 5.5417e-06 1.70533e-05 0.000271465 0.998583 0.000443651 0.000135738 0.000285967 0.000131224\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "540\n",
      "2.81422e-06 9.38282e-06 1.71578e-06 4.81095e-06 0.000249716 0.999584 4.72713e-05 1.7072e-05 4.39037e-05 3.97207e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000003 training accuracy 1.000000\n",
      "-------------\n",
      "550\n",
      "1.42476e-05 5.5699e-05 3.65527e-06 2.09452e-05 0.000193832 0.999081 0.000387595 3.76451e-05 0.000148156 5.68077e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "560\n",
      "1.72387e-05 6.87126e-05 3.33222e-06 1.29193e-05 0.000164335 0.999058 0.000370631 5.10088e-05 0.000129433 0.000124539\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "570\n",
      "6.25819e-06 6.9957e-05 2.13017e-06 6.79695e-06 0.000136695 0.99946 0.000125305 2.62774e-05 0.000121568 4.50749e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000004 training accuracy 1.000000\n",
      "-------------\n",
      "580\n",
      "1.10593e-05 6.03655e-05 2.83919e-06 1.65194e-05 0.000158021 0.999348 0.000159755 4.54766e-05 0.000113552 8.49438e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "590\n",
      "6.77016e-06 5.49273e-05 2.17117e-06 4.13433e-06 0.000112162 0.999372 0.000139674 0.000127101 0.000115185 6.62012e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000007 training accuracy 1.000000\n",
      "-------------\n",
      "600\n",
      "1.85928e-05 9.11693e-05 1.05718e-05 0.000126862 0.00116243 0.99815 8.83051e-05 0.000106119 0.000134246 0.000111836\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "610\n",
      "5.95992e-06 8.24635e-05 1.00927e-05 1.07883e-05 0.00140298 0.997983 0.000161847 0.000162576 8.9569e-05 9.04822e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000012 training accuracy 1.000000\n",
      "-------------\n",
      "620\n",
      "9.00675e-06 4.03804e-05 2.69478e-06 1.96338e-05 0.000236574 0.999505 6.98922e-05 5.0566e-05 3.6392e-05 2.96509e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "630\n",
      "3.34771e-06 6.85097e-05 3.32775e-06 1.52446e-05 0.000269862 0.99943 4.85129e-05 3.06152e-05 8.72364e-05 4.2876e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "640\n",
      "4.19538e-06 6.48415e-05 2.83256e-06 1.88313e-05 0.000221751 0.999493 9.24091e-05 2.0828e-05 5.35181e-05 2.76445e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "650\n",
      "2.16676e-06 1.71529e-05 1.36894e-06 1.78629e-06 0.000130067 0.999725 6.06643e-05 1.15085e-05 2.94415e-05 2.06555e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "660\n",
      "3.77297e-06 2.36666e-05 2.19116e-06 3.69484e-06 0.000101607 0.999572 0.000163736 2.20076e-05 5.43921e-05 5.34341e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "670\n",
      "5.45736e-06 6.49782e-05 2.99108e-06 8.54274e-06 0.000233396 0.999432 0.000103731 3.20993e-05 4.87839e-05 6.78302e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "680\n",
      "1.84826e-05 6.18033e-05 7.70318e-06 4.92644e-05 0.000281818 0.998319 0.000547146 8.06024e-05 0.000141558 0.000492788\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "690\n",
      "9.85231e-06 7.0623e-05 4.6787e-06 3.21248e-05 0.000399091 0.998978 0.000101752 8.0874e-05 0.000217905 0.000104854\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "700\n",
      "1.01796e-05 3.67775e-05 1.34398e-05 4.04626e-05 0.000246137 0.998952 0.000142643 9.63861e-05 0.000290386 0.000171559\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "710\n",
      "5.19824e-06 2.34995e-05 3.34941e-06 1.79262e-05 0.000241465 0.999114 0.000246945 8.71721e-05 0.000154703 0.00010594\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "720\n",
      "4.08166e-06 4.34428e-05 2.19673e-06 3.0646e-05 0.000145983 0.999488 9.83423e-05 3.32583e-05 0.000122244 3.14338e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "730\n",
      "2.86233e-06 3.22015e-05 1.83326e-06 9.30858e-06 0.00010366 0.999583 0.000103166 2.11232e-05 0.000113017 2.98133e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "740\n",
      "1.1992e-06 2.09476e-05 1.46124e-06 2.46797e-06 8.15229e-05 0.999723 6.52156e-05 1.2467e-05 6.04441e-05 3.0761e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "750\n",
      "1.30388e-06 1.4938e-05 9.38443e-07 2.15896e-06 0.000115554 0.999579 0.000106036 2.15873e-05 0.000128912 2.99145e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "760\n",
      "1.83517e-06 2.83589e-05 1.06739e-06 1.09828e-05 0.000109475 0.999703 5.46516e-05 1.75757e-05 5.79838e-05 1.52254e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "770\n",
      "3.10858e-06 3.37662e-05 2.71476e-06 8.26732e-06 0.000230077 0.999155 0.000427039 4.30117e-05 4.87131e-05 4.87502e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "780\n",
      "2.53865e-07 1.02718e-05 5.78929e-07 5.34891e-07 4.94949e-05 0.999902 2.53169e-06 6.6673e-06 2.26672e-05 4.5319e-06\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790\n",
      "1.00984e-06 4.18392e-05 2.70057e-06 4.0714e-06 0.000107208 0.9996 9.40749e-05 2.42825e-05 5.60564e-05 6.93687e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "800\n",
      "6.18936e-07 1.3757e-05 1.13606e-06 2.04933e-06 6.68137e-05 0.999834 3.82116e-05 7.11548e-06 1.46461e-05 2.16774e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "810\n",
      "1.42189e-06 1.37639e-05 7.69119e-07 3.54977e-06 9.40323e-05 0.999821 3.75605e-05 7.16721e-06 1.04719e-05 1.02597e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "820\n",
      "8.44255e-07 1.15351e-05 7.17365e-07 2.07899e-06 8.85244e-05 0.999826 2.96687e-05 6.82803e-06 2.13957e-05 1.25096e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "830\n",
      "5.17583e-07 7.56844e-06 7.38737e-07 5.20805e-07 3.03288e-05 0.999839 6.44522e-05 5.67359e-06 2.85765e-05 2.26936e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "840\n",
      "5.12774e-07 8.33419e-06 5.4926e-07 4.70085e-07 2.66369e-05 0.999857 4.60054e-05 5.17794e-06 3.15682e-05 2.37348e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "850\n",
      "3.00705e-07 7.60656e-06 2.54095e-07 3.73791e-07 3.05016e-05 0.999887 1.71867e-05 8.32524e-06 1.34568e-05 3.49554e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "860\n",
      "6.89838e-06 1.78095e-05 2.86648e-06 1.35137e-05 5.44843e-05 0.999768 7.25004e-05 2.17623e-06 3.89478e-05 2.31547e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "870\n",
      "2.26778e-06 2.3748e-05 3.52737e-06 6.97294e-06 4.54406e-05 0.999574 0.000134737 3.17081e-06 0.000184728 2.15155e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "880\n",
      "2.46526e-06 1.44542e-05 1.19668e-06 1.0788e-05 0.000181486 0.999712 2.09982e-05 7.28231e-06 3.01992e-05 1.92219e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "890\n",
      "9.2412e-08 4.68324e-06 6.35151e-08 8.42756e-08 1.31911e-05 0.999914 4.21801e-05 3.33659e-06 1.30251e-05 9.66421e-06\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "900\n",
      "2.8394e-07 2.35955e-05 3.42478e-07 3.21842e-07 3.01828e-05 0.999881 2.51914e-05 3.20892e-06 2.68744e-05 8.68791e-06\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000001 training accuracy 1.000000\n",
      "-------------\n",
      "910\n",
      "5.67685e-07 3.30752e-05 1.79447e-06 2.3897e-06 7.30455e-05 0.999525 0.000303994 6.97778e-06 3.33361e-05 1.96487e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "920\n",
      "3.38051e-07 1.12883e-05 5.4069e-07 1.29433e-06 5.06477e-05 0.999883 1.8985e-05 1.40748e-06 2.13462e-05 1.08556e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "930\n",
      "5.8467e-07 5.11673e-06 3.53043e-07 1.80328e-06 2.1494e-05 0.999909 2.35505e-05 1.62473e-06 2.57335e-05 1.10704e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "940\n",
      "1.619e-05 2.19288e-05 3.88688e-08 1.60401e-06 6.05103e-05 0.999841 3.90241e-05 1.25627e-05 1.93548e-06 5.38504e-06\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000003 training accuracy 1.000000\n",
      "-------------\n",
      "950\n",
      "1.15119e-05 0.00020016 1.08528e-05 2.25458e-05 4.6782e-05 0.997854 0.0010755 0.000108405 0.000511572 0.000158959\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "960\n",
      "9.3263e-07 3.74217e-05 2.95211e-07 3.71265e-06 3.54432e-05 0.999718 1.58987e-05 5.72221e-06 0.000146349 3.61154e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000002 training accuracy 1.000000\n",
      "-------------\n",
      "970\n",
      "3.21252e-06 1.25037e-05 1.52692e-07 1.33181e-05 9.72167e-05 0.999817 1.54346e-05 9.34201e-06 1.69086e-05 1.46027e-05\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "980\n",
      "3.18184e-07 1.09834e-05 1.5565e-07 2.52446e-06 3.84386e-05 0.999904 1.12964e-05 3.94063e-06 1.87326e-05 9.87057e-06\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "990\n",
      "2.83767e-07 9.95502e-06 1.22105e-07 1.85282e-06 3.13965e-05 0.999916 8.86498e-06 3.50607e-06 1.94573e-05 8.98386e-06\n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
      "loss 0.000000 training accuracy 1.000000\n",
      "-------------\n",
      "---end---\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #执行初始化\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    for epoch in range(1000):\n",
    "        for i in range(8):\n",
    "            offset = i * BATCH_SIZE\n",
    "            batch_xs = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "#             print(batch_xs.shape)\n",
    "            batch_xs = batch_xs.reshape(-1, DATA_SIZE)\n",
    "            batch_ys = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "            \n",
    "            summary,_ = sess.run([merged,train_step],feed_dict={x:batch_xs, y:batch_ys, k: 0.8})\n",
    "            _y = sess.run(y_out, feed_dict={x:batch_xs, y:batch_ys, k: 1.0})\n",
    "            \n",
    "#             print(i, _y.shape[0], _y[0][0])\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch)\n",
    "            train_accuacy = accuracy.eval(feed_dict={x:batch_xs, y:batch_ys, k: 1.0})\n",
    "            _y = sess.run(y_out, feed_dict={x:batch_xs, y:batch_ys, k: 1.0})\n",
    "            _loss = sess.run(loss, feed_dict={x:batch_xs, y:batch_ys, k: 1.0})\n",
    "            print(\"%s %s %s %s %s %s %s %s %s %s\"%(_y[0][0],_y[0][1],_y[0][2],_y[0][3],_y[0][4],_y[0][5],_y[0][6],_y[0][7],_y[0][8],_y[0][9]))\n",
    "            print(\"%s %s %s %s %s %s %s %s %s %s\"%(batch_ys[0][0],batch_ys[0][1],batch_ys[0][2],batch_ys[0][3],batch_ys[0][4],batch_ys[0][5],batch_ys[0][6],batch_ys[0][7],batch_ys[0][8],batch_ys[0][9]))\n",
    "            print(\"loss %f training accuracy %f\"%(_loss, train_accuacy))\n",
    "            print(\"-------------\")\n",
    "    print(\"---end---\")\n",
    "    graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['layer/out/p/output'])\n",
    "    tf.train.write_graph(graph, '.', 'num.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
